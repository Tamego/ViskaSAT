== CDCLの基礎
=== 概要
単位伝播によってDPLLでは探索空間を削減していたが、
ここではさらなる効率化を行う。

以下のCNFを考えてみよう：
$
(not x_1 or x_2) and (not x_3 or x_5) and (not x_4 or x_5) and (not x_6 or not x_7)
\ and (not x_1 or not x_5 or x_6) and (not x_2 or not x_5 or x_7)
$
これをDPLLで解いてみると次のようなステップになる：
1. ${x_1 mapsto "true"}$ と決定する。
2. 単位伝播により ${x_2 mapsto "true"}$。
3. ${x_3 mapsto "true"}$ と決定する。
4. 単位伝播により ${x_5 mapsto "true", x_6 mapsto "true", x_7 mapsto "true"}$。
5. $not x_6 or not x_7$ が非充足となり矛盾。
  $x_3, x_5, x_6, x_7$ の割り当てを削除する。
6. ${x_3 mapsto "false"}$ と決定する。
7. ${x_4 mapsto "true"}$ と決定する。
8. 単位伝播により ${x_5 mapsto "true", x_6 mapsto "true", x_7 mapsto "true"}$。
9. $not x_6 or not x_7$ が非充足となり矛盾。
  $x_4, x_5, x_6, x_7$ の割り当てを削除する。
10. （以下省略）
ここで4, 5と8, 9で行なっている作業はほとんど同じである。
4, 5の経験を活かして8, 9の探索を削減できることが望ましい。
矛盾の原因を学習して同じ間違いを繰り返さないようにする仕組みを備えたものがCDCLソルバである。

CDCLのコンセプトを説明する前に基本的な用語を定義する：
/ 決定レベル: 各変数における割り当てがどの決定に結び付いているかを表す。
  たとえば決定レベル1の変数は、1回目の決定に由来するということを表す。
/ バックトラック: 指定された決定レベルより大きい決定レベルの変数の割り当てを削除すること。

CDCLの*学習*の基本原理として*導出原理*という法則がある。
リテラル $a_1, a_2, dots, a_m$, $b_1, b_2, dots, b_n$, $alpha$ について、
$
(a_1 or a_2 or dots or a_m or alpha) and (b_1 or b_2 or dots or b_n or not alpha)
$
#h(-1em)から
$
a_1 or a_2 or dots or a_m or b_1 or b_2 or dots or b_n
$
#h(-1em)が導かれるという法則のことである。

ここで、矛盾の根本的な原因を考察してみる。
決定レベルが $d$ であるような決定をして、その後の単位伝播によって、節 $c$ が矛盾することが分かった。
ここで、決定レベルが $d - 1$ の時点では矛盾する節はないので、
節 $c$ には決定レベル $d$ で決定されたリテラルとそれに由来する伝播されたリテラルが含まれている。
節 $c$ に含まれる決定レベル $d$ で伝播されたリテラルを $alpha$ とおく。
すると、リテラル $a_1, a_2, dots, a_m$ を用いて節 $c$ は次のように書ける：
$
c = a_1 or a_2 or dots or a_m or not alpha
$
#h(-1em)ここで、決定レベル $d$ において $alpha$ が伝播されていることから、
その伝播の理由となる単位節が存在する。
その単位節を $u$ として、リテラル $b_1, b_2, dots, b_n$ を用いて節 $u$ は次のように書ける：
$
u = b_1 or b_2 or dots or b_n or alpha
$
#h(-1em)したがって、導出原理により以下の節 $l$ が導かれる：
$
l = a_1 or a_2 or dots or a_m or b_1 or b_2 or dots or b_n
$
#h(-1em)ここで、この節を式に加えても充足可能性は変化しないので、
この得られた節を*学習節*として加えても問題ない。
この一連の操作によって学習節を生成することを*学習*という。

この得られた学習節の意味を考えてみよう。
もともと、矛盾の原因はリテラル $a_1, a_2, dots, a_m$ と $not alpha$ が全て充足しないことであった。
そもそも $not alpha$ が充足しない原因は $alpha$ が充足することであって、
それは単位節 $u$ による伝播が原因であった。
それは節 $u$ が単位節になること、つまり $b_1, b_2, dots, b_n$ が全て充足しないことである。
だから結局 $a_1, a_2, dots, a_m$ と $b_1, b_2, dots, b_n$ が全て充足しないことが矛盾の原因である。
よって、学習節として、これらのリテラルのどれかが充足するという条件を表す節 $l$ が得られる。
こうして、$alpha$ を使わずに矛盾の原因を表現することができた。

ここで注目したいのは、節 $c$ に含まれる決定レベル $d$ で伝播されたリテラルであれば同じ操作ができて、
そのリテラルを使わない形で矛盾の原因を表現することができるということである。
この操作をできる限り繰り返すことで、
決定レベルが $d$ であるようなリテラルがただ1つのみ含まれるような学習節 $l'$ を必ず作ることができる。
そのリテラルを $beta$ とする。
すると、現在の割り当てでは $l'$ 内のリテラルは全て充足しないことを踏まえれば、
$l'$ のリテラルの決定レベルの中で2番目に大きい決定レベル、
つまり決定レベルが $d$ でないもののうち最も大きい決定レベルにバックトラックすると
節 $l'$ はただちに単位節となる。
したがって、すぐに $beta$ が伝播される。
矛盾の原因を解析することで大幅に探索空間が削減されることが分かる。
この解析を*矛盾の解析*などと呼んだりする。

この一連の操作は*含意グラフ*を用いることで視覚的に捉えることができる。
ただし、この部分はビジュアライザのドキュメントの方に譲ろうと思う。

経験的に、矛盾節に含まれる決定レベル $d$ のリテラルであって、
最も直近に割り当てられたものから順番に導出原理を適用させることで、
得られる学習節が短く効果的になることが知られているそう。
実装が簡便であることから、今回はこの方針を取る。

実際に具体例を通してどのようにCDCLの学習が効果的に働くかを見てみよう。
以下のように節に名前を付けて、その連言を考える。
$
c_1 = not x_1 or x_2,quad c_2 = not x_3 or x_5,\
c_3 = not x_4 or x_5,quad c_4 = not x_6 or not x_7,\
c_5 = not x_1 or not x_5 or x_6,quad c_6 = not x_2 or not x_5 or x_7\
$
- 決定（レベル1）
  - ${x_1 mapsto "true"}$ と決定する。
  - 単位伝播により ${x_2 mapsto "true"}$。
- 決定（レベル2）
  - ${x_3 mapsto "true"}$ と決定する。
  - 単位伝播により ${x_5 mapsto "true", x_6 mapsto "true", x_7 mapsto "true"}$。
  - $c_4$ が非充足となり矛盾。
- 矛盾の解析
  - 矛盾節は $not x_6 or not x_7$
  - $x_7$ の伝播理由は $c_6$。
    ここで導出原理により、$not x_2 or not x_5 or not x_6$。
  - $x_6$ の伝播理由は $c_5$。
    ここで導出原理により、$not x_1 or not x_2 or not x_5$
  - 決定レベルが2であるリテラルが $not x_5$ になったので終了。
    これを学習節 $c_7 = not x_1 or not x_2 or not x_5$ とする。
- バックトラック
  - $c_7$ において2番目に大きい決定レベルは1なので、決定レベル1にバックトラックする。
  - 割り当ては ${x_1 mapsto "true", x_2 mapsto "true"}$。
  - 単位伝播により、${x_5 mapsto "false", x_6 mapsto "false", x_7 mapsto "false"}$
- （以下省略）
これをDPLLと比較すれば、学習節によって、直ちに ${x_5 mapsto "false"}$ が結論付けらていて、
無駄な探索が減っていることが分かる。
